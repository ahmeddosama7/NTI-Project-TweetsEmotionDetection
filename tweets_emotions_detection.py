# -*- coding: utf-8 -*-
"""Tweets Emotions detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V6EQwnlBX89fvfgQ0NgMpNJJ11vz_yit
"""

import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

import nltk, emoji
nltk.download('stopwords', quiet=True)

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

TRAIN_TXT = "/content/train.txt"
VAL_TXT   = "/content/val.txt"
TEST_TXT  = "/content/test.txt"

import pandas as pd

def read_split(path):
    df = pd.read_csv(path, sep=None, engine="python", header=None, names=["text","label"])
    df["text"]  = df["text"].astype(str).str.strip().str.strip('"').str.strip()
    df["label"] = df["label"].astype(str).str.strip().str.strip('"').str.strip()
    return df

train_df = read_split(TRAIN_TXT)
val_df   = read_split(VAL_TXT)
test_df  = read_split(TEST_TXT)

print("Train:", train_df.shape, "\nVal  :", val_df.shape, "\nTest :", test_df.shape)

train_df.head()

import re, emoji
from nltk.corpus import stopwords
STOP_EN = set(stopwords.words("english"))

def clean_tweet(s: str) -> str:
    if not isinstance(s, str): return ""
    s = s.strip()
    s = re.sub(r"http\S+|www\.\S+", " ", s)
    s = re.sub(r"@\w+", " ", s)
    s = re.sub(r"\brt\b", " ", s, flags=re.IGNORECASE)
    s = re.sub(r"#(\w+)", r"\1", s)
    s = emoji.demojize(s).replace(":", " ").replace("_", " ")
    s = re.sub(r"[^a-zA-Z0-9\s!?.,']", " ", s).lower()
    s = re.sub(r"(.)\1{2,}", r"\1\1", s)
    s = re.sub(r"\s+", " ", s).strip()
    toks = [t for t in s.split() if (t not in STOP_EN or t in {"no","not","nor"})]
    return " ".join(toks)

for df_ in (train_df, val_df, test_df):
    df_["text_clean"] = df_["text"].astype(str).map(clean_tweet)

train_df[["text","text_clean","label"]].head(8)

def describe_split(name, df_):
    print(f"{name}:", df_.shape)
    print(df_["label"].value_counts(), "\n")

describe_split("Train", train_df)
describe_split("Val",   val_df)
describe_split("Test",  test_df)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
import pickle

le = LabelEncoder()
y_tr = le.fit_transform(train_df["label"])
y_va = le.transform(val_df["label"])
y_te = le.transform(test_df["label"])

#TF-IDF Fitting
tfidf = TfidfVectorizer(token_pattern=r"(?u)\b\w+\b", ngram_range=(1,2), min_df=2, max_df=0.9)
X_tr = tfidf.fit_transform(train_df["text_clean"])
X_va = tfidf.transform(val_df["text_clean"])
X_te = tfidf.transform(test_df["text_clean"])

#LinearSVC Training
svc = LinearSVC()
svc.fit(X_tr, y_tr)

#Validation
va_pred = svc.predict(X_va)
print("VALID acc:", accuracy_score(y_va, va_pred))
print(classification_report(y_va, va_pred, target_names=le.classes_))

#Confusion Matrix
te_pred = svc.predict(X_te)
print("TEST acc:", accuracy_score(y_te, te_pred))
print(classification_report(y_te, te_pred, target_names=le.classes_))

cm = confusion_matrix(y_te, te_pred)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.title("Confusion Matrix ‚Äî TF-IDF + LinearSVC (Test)")
plt.xlabel("Predicted"); plt.ylabel("True"); plt.show()

import pickle, os
pickle.dump(tfidf, open("/content/tfidf.pkl","wb"))
pickle.dump(le,    open("/content/label_encoder.pkl","wb"))
pickle.dump(svc,   open("/content/svc.pkl","wb"))
print("Saved ‚Üí /content/tfidf.pkl, /content/label_encoder.pkl, /content/svc.pkl")

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping

tok = Tokenizer(oov_token="<unk>")
tok.fit_on_texts(train_df["text_clean"])
MAX_LEN = 48

Xtr_seq = pad_sequences(tok.texts_to_sequences(train_df["text_clean"]), maxlen=MAX_LEN, padding="post")
Xva_seq = pad_sequences(tok.texts_to_sequences(val_df["text_clean"]),   maxlen=MAX_LEN, padding="post")
Xte_seq = pad_sequences(tok.texts_to_sequences(test_df["text_clean"]),  maxlen=MAX_LEN, padding="post")

num_classes = len(le.classes_)
model = models.Sequential([
    layers.Embedding(input_dim=len(tok.word_index)+1, output_dim=128, mask_zero=True, input_length=MAX_LEN),
    layers.Bidirectional(layers.LSTM(128, return_sequences=True)),
    layers.Dropout(0.25),
    layers.Bidirectional(layers.LSTM(64)),
    layers.Dropout(0.25),
    layers.Dense(128, activation="relu"),
    layers.Dense(num_classes, activation="softmax")
])
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])

es = EarlyStopping(monitor="val_loss", patience=3, restore_best_weights=True)
hist = model.fit(Xtr_seq, le.transform(train_df["label"]),
                 validation_data=(Xva_seq, le.transform(val_df["label"])),
                 epochs=10, batch_size=128, callbacks=[es], verbose=1)

te_pred_nn = model.predict(Xte_seq).argmax(axis=1)
print("BiLSTM TEST acc:", accuracy_score(le.transform(test_df["label"]), te_pred_nn))
print(classification_report(le.transform(test_df["label"]), te_pred_nn, target_names=le.classes_))

model.save("/content/bilstm_emotion.h5")
pickle.dump(tok, open("/content/tokenizer.pkl","wb"))
print("Saved ‚Üí /content/bilstm_emotion.h5, /content/tokenizer.pkl")

layers.Embedding(input_dim=len(tok.word_index)+1, output_dim=128, mask_zero=True)

model.save("/content/bilstm_emotion.keras")

import pickle
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf

def clean_for_infer(texts):
    return [clean_tweet(t) for t in texts]

def predict_baseline(texts):
    vec = pickle.load(open("/content/tfidf.pkl","rb"))
    enc = pickle.load(open("/content/label_encoder.pkl","rb"))
    mod = pickle.load(open("/content/svc.pkl","rb"))
    X = vec.transform(clean_for_infer(texts))
    y = mod.predict(X)
    return enc.inverse_transform(y)

def predict_bilstm(texts):
    tok = pickle.load(open("/content/tokenizer.pkl","rb"))
    enc = pickle.load(open("/content/label_encoder.pkl","rb"))
    mdl = tf.keras.models.load_model("/content/bilstm_emotion.h5")
    X = pad_sequences(tok.texts_to_sequences(clean_for_infer(texts)), maxlen=48, padding="post")
    y = mdl.predict(X).argmax(axis=1)
    return enc.inverse_transform(y)

#Demo
samples = ["I am so happy today! üòä", "This is disgusting.", "I'm worried about tomorrow.", "so angry rn"]
print("Baseline:", list(predict_baseline(samples)))
try:
    print("BiLSTM :", list(predict_bilstm(samples)))
except Exception as e:
    print("BiLSTM not available yet:", e)

!pip -q install -U gradio

import gradio as gr
import pickle, re, emoji

#Load
VEC = pickle.load(open("/content/tfidf.pkl","rb"))
ENC = pickle.load(open("/content/label_encoder.pkl","rb"))
SVC = pickle.load(open("/content/svc.pkl","rb"))

def clean_tweet(s: str) -> str:
    if not isinstance(s, str): return ""
    s = re.sub(r"http\S+|www\.\S+", " ", s)
    s = re.sub(r"@\w+", " ", s)
    s = re.sub(r"\brt\b", " ", s, flags=re.I)
    s = re.sub(r"#(\w+)", r"\1", s)
    s = emoji.demojize(s).replace(":", " ").replace("_", " ")
    s = re.sub(r"[^a-zA-Z0-9\s!?.,']", " ", s).lower()
    s = re.sub(r"(.)\1{2,}", r"\1\1", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def predict_emotion(text: str):
    txt = clean_tweet(text)
    X = VEC.transform([txt])
    y = SVC.predict(X)
    return ENC.inverse_transform(y)[0]

# Example tweets
examples = [
    ["I can‚Äôt stop smiling today, this is the best day of my life!"],
    ["This is absolutely ridiculous, I‚Äôm furious right now."],
    ["I feel so alone and empty, nothing seems to help."],
    ["I‚Äôm terrified about tomorrow‚Äôs exam."],
    ["So proud of you, you make my heart warm ‚ù§Ô∏è"],
    ["Wait‚Ä¶ it actually worked?! No way üò±"],
]

#Interface
demo = gr.Interface(
    fn=predict_emotion,
    inputs=gr.Textbox(lines=3, label="Enter a tweet"),
    outputs=gr.Label(label="Predicted Emotion"),
    title="Emotion Detection in Tweets",
    description="Baseline model: TF-IDF + LinearSVC",
    examples=examples
)
demo.queue()
demo.launch(inline=True, debug=True)